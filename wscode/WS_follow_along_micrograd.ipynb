{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f06fd8-0727-45a2-a08d-4ea33dddeea5",
   "metadata": {},
   "source": [
    "# WS_follow_along_micrograd\n",
    "# WESmith 06/01/23\n",
    "## follow along with Karpathy video https://www.youtube.com/watch?v=VMj-3S1tku0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3159ec9-bd3e-441a-8ae5-6c76ad58d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "from numpy import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dbc4ae-0456-4b2f-b17a-aceeb2a2c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data  = data\n",
    "        self.grad  = 0.0\n",
    "        self._backward = lambda: None  # empty function for a leaf node\n",
    "        self._prev = set(_children)\n",
    "        self._op   = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        # WS handle adding integers\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out   = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            # WS note: must accumulate gradients to handle repetitive\n",
    "            # appearance of a variable (discussed in the video at 1:25:40),\n",
    "            # due to basic calculus rules for multivariate derivatives\n",
    "            self.grad  += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward   = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), 'only supporting int/float powers for now'\n",
    "        out = Value(self.data**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * self.data**(other - 1) * out.grad\n",
    "        out._backward  = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        # WS handle multiplying by an integer\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out   = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad  += other.data * out.grad\n",
    "            other.grad +=  self.data * out.grad\n",
    "        out._backward   = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "    \n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**(-1)\n",
    "    \n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**(-1)\n",
    "    \n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self, ), 'ReLU')\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward  = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        v = math.exp(2 * x)\n",
    "        t = (v - 1) / (v + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "        # WS note: _backward() has built into it the 'self' pointer to the\n",
    "        # child node to set its grad value, the 't' value of the parent node,\n",
    "        # and the 'out' pointer to the parent node: it is simple and works nicely;\n",
    "        # this is a powerful use of a python function to encapsulate all of this\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward  = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward  = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self, number=False): # WS this is called on the final node\n",
    "        # WS added 'number' option, to number nodes from the final node down\\\n",
    "        # number = True: just number the nodes\n",
    "        # number = False (default): calculate gradients only, no numbering of nodes\n",
    "        topo    = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        if number: # WS addition\n",
    "            val = 0\n",
    "            for node in reversed(topo):\n",
    "                node.label = str(val)\n",
    "                val += 1\n",
    "        else:\n",
    "            self.grad = 1.0\n",
    "            for node in reversed(topo): # WS calculate all the gradients\n",
    "                node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191811f-ef04-4f3f-95a7-1a5eff76e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module: # to keep similarity to pytorch\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "            \n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.nin = nin # WS added\n",
    "        self.w   = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b   = Value(random.uniform(-1, 1))\n",
    "        self.nonlin = nonlin # apply a nonlinearity or not to neuron output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        # WS a clever way to use zip()\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return act.tanh() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b] # list plus list\n",
    "\n",
    "    def __repr__(self): # WS added\n",
    "        #return f\"Neuron(num_inputs={self.nin})\"\n",
    "        return f\"{'Tanh ' if self.nonlin else 'Linear '}N({len(self.w)}{' inputs'})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.nout    = nout  # WS added\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        '''\n",
    "        params = []\n",
    "        for n in self.neurons:\n",
    "            ps = n.parameters()\n",
    "            params.extend(ps)\n",
    "        return params\n",
    "        '''\n",
    "        # WS below from video: equivalent to the above code\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self): # WS added\n",
    "        #return f\"Layer(num_neurons={self.nout})\"\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts): # WS nouts is a list\n",
    "        self.nin = nin  # WS added\n",
    "        sz = [nin] + nouts\n",
    "        # make the last layer Linear\n",
    "        self.layers = [Layer(sz[i], sz[i + 1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    " \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self): # WS added\n",
    "        #return f\"MLP(num_inputs={self.nin}, layers={self.layers})\"\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c10d9-ae2b-445e-991c-53b4ad1ad626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topo(val): # WS addition\n",
    "    topo    = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            for child in v._prev:\n",
    "                build_topo(child)\n",
    "            topo.append(v)\n",
    "\n",
    "    build_topo(val)\n",
    "    #for node in reversed(topo):\n",
    "    #    print(node.label, node._op, node.data, node.grad)\n",
    "    for k in reversed(topo): # WS\n",
    "        print('label: {:15s} op: {:5s} value: {:7.4f} grad: {:7.4f}'.format(k.label, k._op, k.data, k.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a69c6-c689-403b-b017-2bdbc9748606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # left to right\n",
    "    \n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n)) # WS built-in id() returns the identity of the object\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name=uid, label='{%s | data %.4f | grad %.4f}' %\\\n",
    "                 (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "            \n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f562b51-3112-4a0d-b48d-2d05065b469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "a / b, a.exp(), a + 1, a * 2, 3 * a  # __rmul__ handles integer * Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dde14-4b75-4671-b701-71985a1d3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - a, -1 - a, a - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95150e56-ee33-488b-a1bc-79c9085fb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a - b\n",
    "draw_dot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d644070-b935-4c7f-9160-c68d7f9206a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward(number=True) # WS test new numbering option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2a74f-5f27-41e2-aefc-d32bcfc217fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd1f56-2651-4b61-b2ab-a22e8bec3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_topo(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575889b-4c2d-45e8-92a6-11609811396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = a/b\n",
    "d.backward(number=True)\n",
    "draw_dot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfe196-a7d3-4b69-88c8-d4cbd239d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value( 2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label='e'\n",
    "d = e + c; d.label='d'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label='L'\n",
    "L.grad = 1 # root node\n",
    "a + b, a * b, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bfc53-b032-45e3-bb8c-71b0162e9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.__add__(b), a.__mul__(b)  # equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d225204-f88f-4890-b406-8390b8968d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "d._prev, d._op, type(d._prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd421e9-79c6-4e6f-afef-c1853417646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward(number=True)\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04975714-5bac-422a-b05c-ce940ba47cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_topo(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58c6dc-36f5-4316-8d99-e969c90eed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.2)\n",
    "plt.plot(x, np.tanh(x)); plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7f245-8460-4306-9c12-459c9db4e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Value( 2.0, label='x1')\n",
    "x2 = Value( 0.0, label='x2')\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value( 1.0, label='w2')\n",
    "b  = Value( 6.8813736, label='b')\n",
    "x1w1 = x1 * w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2 * w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n    = x1w1x2w2 + b; n.label = 'n'\n",
    "o    = n.tanh(); o.label = 'o'\n",
    "# calculate gradients: \n",
    "# NOTE if backward() is run consecutively without redefining the Value object, \n",
    "# the gradients aren't zeroed out each time, and the results are incorrect since they are cumulative\n",
    "# (this is handled with the zero_grad() method of the Neuron, Layer, and MLP classes, however)\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca2945-1b18-4ba4-95fa-8eea1e0ea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244186c-434f-4f6c-ab55-7d3c715f272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_topo(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3877c9-966a-44ac-8443-ae9a86b3ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a  # handle duplicate entries by accumulating grads in Value class\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ad7fa-6c54-474c-9213-152880884c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value( 3.0, label='b')\n",
    "d = a * b; d.label = 'd'\n",
    "e = a + b; e.label = 'e'\n",
    "f = d * e; f.label = 'f'\n",
    "f.backward()\n",
    "dot = draw_dot(f)\n",
    "dot\n",
    "# f = a**2 * b + a * b**2, df/da = 2*a*b + b**2, df/db = a**2 + 2*a*b\n",
    "# for a = -2, b = 3, f = 12 - 18 = -6, df/da = -12 + 9 = -3, df/db = 4 - 12 = -8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df66b9-fe39-4f2f-8807-2212fb9d85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dot.source) # see https://graphviz.readthedocs.io/en/stable/manual.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033af44-5b5d-4b21-ba2d-2e3755a0c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://graphviz.readthedocs.io/en/stable/api.html?highlight=render#graphviz.Digraph.render\n",
    "#dot.render(filename='test_graph', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f87e6-1d69-4f05-9d38-69956b8d214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Value( 2.0, label='x1')\n",
    "x2 = Value( 0.0, label='x2')\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value( 1.0, label='w2')\n",
    "b  = Value( 6.8813736, label='b')\n",
    "x1w1 = x1 * w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2 * w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n    = x1w1x2w2 + b; n.label = 'n'\n",
    "# explicitly implement tanh now:\n",
    "e    = (2*n).exp();     e.label = 'e'\n",
    "o    = (e - 1)/(e + 1); o.label = 'o'\n",
    "o.backward()\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ed1ae-7065-4391-9dc7-3dc133b9dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = draw_dot(o)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821fd2d-210d-40eb-8d08-dbb148bd44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e94b2e-84d8-48e2-bf7e-72666889c1af",
   "metadata": {},
   "source": [
    "# PyTorch comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1cc7c3-7946-4f37-b74c-ddbad2ac45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.Tensor([ 2.0]).double();      x1.requires_grad = True\n",
    "x2 = torch.Tensor([ 0.0]).double();      x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double();      w1.requires_grad = True\n",
    "w2 = torch.Tensor([ 1.0]).double();      w2.requires_grad = True\n",
    "b  = torch.Tensor([6.8813736]).double();  b.requires_grad = True\n",
    "n  = x1 * w1 + x2 * w2 + b\n",
    "o  = torch.tanh(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff89b06-67d4-45d1-9cde-a75f73d0fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86194bc-0075-48e4-837c-35b7216fc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35867c17-6ac0-451d-bbb7-fd97cb16fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcc8b009-33cb-47c2-bcca-2d42befb6f99",
   "metadata": {},
   "source": [
    "draw_dot(o) # this fails for torch objects: no _prev attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc33c8c-4683-4d09-8f06-147ca8ed8bf1",
   "metadata": {},
   "source": [
    "## ASIDE: double-list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fd7a1-6f32-4220-971f-a8067db5bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WS example of double-list comprehension\n",
    "data = [[1,2,3],[7,8,9]]\n",
    "# for loops\n",
    "out1 = []\n",
    "for k in data:\n",
    "    for j in k:\n",
    "        out1.append(j)\n",
    "# with list comp\n",
    "out2 = [j for k in data for j in k]\n",
    "out1, out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82592f81-429b-4ae4-b0a2-505259e258d1",
   "metadata": {},
   "source": [
    "# GRADIENT-DESCENT EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20603e-ce20-431d-9104-88b27a0d3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLP(3, [4, 4, 1]) # 3 inputs, 4 neurons, 4 neurons, 1 output neuron\n",
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ac64497-56c3-4725-be26-e71449a677f8",
   "metadata": {},
   "source": [
    "for k in n.parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb8e26-f0aa-437d-9a3d-28c368695309",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in n.layers:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6019a8a-9b38-4044-9aad-b088d8264200",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.layers[0].neurons[0].w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7fcba-999d-4a33-b343-3abea0f23641",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[2.0, 3.0, -1.0], \n",
    "      [3.0, -1.0, 0.5],\n",
    "      [0.5, 1.0, 1.0],\n",
    "      [1.0, 1.0, -1.0]] # 4 input vectors\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # 4 desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbcf7a9-bea6-4245-a9b8-3d47260135c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output before training\n",
    "for x in xs:\n",
    "    print(f'input: {x}, output: {n(x).data:7.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc34565-132a-4b25-a97e-26a849b470e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = [n(x) for x in xs] # applying the 'batch' to the neural net\n",
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss  # starting loss before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5434b35-5b1d-4485-bfa9-18c87b508d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss.backward(number=True) # don't calculate gradients, but label nodes\n",
    "dot = draw_dot(loss) # really huge\n",
    "dot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc30b933-60f4-4cf7-9831-af56cd326dac",
   "metadata": {},
   "source": [
    "# this works, saves a single-page pdf with a huge graph\n",
    "dot.render(filename='huge_test', format='pdf', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e0a60-49f1-43f2-bdb4-d16f2c9f0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for k in range(20):\n",
    "    \n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs] # loop over data in a batch xs\n",
    "    loss  = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "    \n",
    "    # backward pass\n",
    "    n.zero_grad() # essential, since gradients are accumulated in each pass\n",
    "    # each Value object of 'loss' has a pointer to its children, which are \n",
    "    # all objects of 'n', so that 'loss' and 'n' reference the same objects\n",
    "    loss.backward()\n",
    "    \n",
    "    # update via gradient descent\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "\n",
    "    print(k, loss.data)\n",
    "#print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb7362-0020-470e-b8f8-fe3e4729cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, x in enumerate(xs):\n",
    "    k = n(x).data\n",
    "    print(f'trained: {k:7.4f}, desired: {ys[j]:4.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec5ca6-5214-434e-8ddc-3acf32fcf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_topo(loss) # a long list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f4fba-f05a-44f5-87f3-f26a5d9d4a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
